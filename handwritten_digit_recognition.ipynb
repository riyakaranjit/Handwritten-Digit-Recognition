{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwritten_digit_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R_H0aSkBPeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4dc8c2ff-6442-4b02-d1a9-18e80ed4a6b8"
      },
      "source": [
        "\"\"\"This is 2 layer neural network that classifies the MNIST dataset. I have used sigmoid as activation function and the ouput layer using softmax classifier \n",
        "causing the loss rate to be loss and the accuracy to be high. The gradient descent used is mini-batch. \n",
        "\"\"\"\n",
        "!pip install mnist\n",
        "import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.18.5)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLwuT1i-BYqL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "463f3fb3-c3e9-4866-bf8a-36dc9b9e3124"
      },
      "source": [
        "train_images=mnist.train_images() #[60000,28,28]\n",
        "train_labels_new=mnist.train_labels() #[60000]\n",
        "test_images=mnist.test_images()   #[10000,28,28]\n",
        "test_labels_new=mnist.test_labels()   #[10000]\n",
        "\n",
        "D=784 #Dimensionality\n",
        "K=10 #[0,1,2,3,..,9] labels\n",
        "\n",
        "print(\"Original training image size:\",train_images.shape) #In order to plot the image we need this original size as flattened size can't be plotted\n",
        "#Flattening the images. Each 28*28 images into a 28*28=784 dimensional vector\n",
        "train_images=train_images.reshape((-1,784)) #3-D to 2-D Rows are not specified but columns are 784\n",
        "test_images=test_images.reshape((-1,784)) \n",
        "print(\"Flatten training image size:\",train_images.shape)\n",
        "print(\"Flatten testing image size:\",test_images.shape)\n",
        "\n",
        "#Normalizing the pixel value from [0,255] to [0.01,1]\n",
        "fac=0.99/255\n",
        "train_images= np.asfarray(train_images[:,:])*fac + 0.01\n",
        "test_images= np.asfarray(test_images[:,:])*fac + 0.01\n",
        "\n",
        "print(\"Min Training image:\",train_images.min())\n",
        "print(\"Max training image:\",train_images.max())\n",
        "\"\"\"\n",
        "#Normalizing the data to keep the gradient manageable. Normize the pixel value from[0,255] to [-0.5,0.5]\n",
        "#print(train_images[2002])\n",
        "train_images=(train_images/255)-0.5\n",
        "#print(train_images[2002])\n",
        "test_images=(test_images/255)-0.5\n",
        "\"\"\"\n",
        "#print(train_labels_new)\n",
        "print(train_labels_new[200])\n",
        "print(train_images.shape)\n",
        "plt.imshow(train_images[200].reshape((28,28)),cmap=plt.cm.binary,interpolation=\"nearest\")\n",
        "plt.axis(\"off\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original training image size: (60000, 28, 28)\n",
            "Flatten training image size: (60000, 784)\n",
            "Flatten testing image size: (10000, 784)\n",
            "Min Training image: 0.01\n",
            "Max training image: 1.0\n",
            "1\n",
            "(60000, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEPElEQVR4nO3d3UlkWRSA0SrtLDQOReNQBLMQzMIHMQypOPzDMAxDtPpp3qxzGW27vmuv9dibAw3NNwdmc+ou1+v1AujZ2fZfAPiYOCFKnBAlTogSJ0T9mpj7X7nw/ZYf/aGbE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEDX1CUC+wcvLy8bZ2dnZ8OzDw8NwvlqthvOTk5PhnA43J0SJE6LECVHihChxQpQ4IUqcEGXPuQWPj48bZ8/Pz8Ozy+XyS3Pmw80JUeKEKHFClDghSpwQJU6IEidE2XNuwXq93jh7e3v79NnFYrE4PT0dzt/f34dzOtycECVOiBInRIkTosQJUeKEKKuULRg969rd3R2enVq1TJ1nPtycECVOiBInRIkTosQJUeKEKHFClD3nFnznk7Gp81OfGLy9vR3O+XvcnBAlTogSJ0SJE6LECVHihChxQpQ95xZs8z2nTwTOh5sTosQJUeKEKHFClDghSpwQJU6Isufcgm2+55w6T4ebE6LECVHihChxQpQ4IUqcECVOiLLn3ILj4+NPzRaLxeL+/n44957z53BzQpQ4IUqcECVOiBInRIkToqxStmB/f3/jbG9vb3jWk7F/h5sTosQJUeKEKHFClDghSpwQJU6IsueMmXrSNTX3ZOzncHNClDghSpwQJU6IEidEiROixAlR9pwxU+8tvef8d7g5IUqcECVOiBInRIkTosQJUeKEKHvOGO85+Y+bE6LECVHihChxQpQ4IUqcECVOiFpOvO/z+C/mq3vQqfecl5eXG2dXV1fDs3zah/9obk6IEidEiROixAlR4oQocUKUVcrM7OyM/3s69WRs6qczR+dfX1+HZ/k0qxSYE3FClDghSpwQJU6IEidEiROi7Dln5rufjI3O393dDc8eHR0N52xkzwlzIk6IEidEiROixAlR4oQocUKUTwDOzFc/AfiV95w3NzfDs/acf5abE6LECVHihChxQpQ4IUqcECVOiLLnnJnDw8Ph/OnpaTifes852oMeHBwMz/JnuTkhSpwQJU6IEidEiROixAlRfhpzZlar1XB+fn4+nPsEYJKfxoQ5ESdEiROixAlR4oQocUKUOCHKk7EfZmqP+ZUnY/xdbk6IEidEiROixAlR4oQocUKUOCHKnnNmpj6zN/Xe8/r6eji/uLj4338nvoebE6LECVHihChxQpQ4IUqcECVOiPK7tbB9frcW5kScECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUb8m5h9+mgz4fm5OiBInRIkTosQJUeKEKHFC1G9tU6pmoDV2vgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fshwvD70BhSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "d29e26f6-6baa-46f3-be29-0d6bab4b7345"
      },
      "source": [
        "#One hot encoding \n",
        "\n",
        "digits=10\n",
        "num_labels=train_labels_new.shape[0] #[60000]\n",
        "num_test_labels=test_labels_new.shape[0] #[10000]\n",
        "\n",
        "print(\"Before one hot encoding:\",train_labels_new)\n",
        "print(\"Training label shape before one hot encoding:\",train_labels_new.shape)\n",
        "print(\"Testing label shape before one hot encoding:\",test_labels_new.shape)\n",
        "\n",
        "train_labels=np.eye(digits)[train_labels_new.astype('int32')]\n",
        "test_labels=np.eye(digits)[test_labels_new.astype(\"int32\")]\n",
        "\n",
        "train_labels=train_labels.reshape(num_labels,digits)\n",
        "test_labels=test_labels.reshape(num_test_labels,digits)\n",
        "print(\"Training labels after one hot encoding:\",train_labels.shape)\n",
        "print(\"Testing labels after one hot encoding:\",test_labels.shape)\n",
        "print(\"After one hot encoding:\\n\",train_labels)\n",
        "\n",
        "print(test_labels_new[20])\n",
        "print(test_labels[20,:])\n",
        "print(np.eye(digits)[test_labels_new.astype('int32')[20]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before one hot encoding: [5 0 4 ... 5 6 8]\n",
            "Training label shape before one hot encoding: (60000,)\n",
            "Testing label shape before one hot encoding: (10000,)\n",
            "Training labels after one hot encoding: (60000, 10)\n",
            "Testing labels after one hot encoding: (10000, 10)\n",
            "After one hot encoding:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "9\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0chD8tLHRNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "aa73309e-449c-44c7-9a87-df40710ac6b0"
      },
      "source": [
        "np.random.seed(300) #the same random number repeats for 300 times\n",
        "\n",
        "num_examples=train_images.shape[0] #60000\n",
        "num_labels=train_labels.shape[0] #60000\n",
        "\n",
        "#hyperparameters\n",
        "reg=1e-3 #regularization strength lambda\n",
        "step_size=4 #learning rate \n",
        "epoch=30\n",
        "mini_batch_size=128\n",
        "batches=num_examples//mini_batch_size #floor division\n",
        "\n",
        "#Neural Network architecture\n",
        "input_layer= 784\n",
        "hidden_layer= 300\n",
        "output_layer= 10\n",
        "\n",
        "#Parameters initialization\n",
        "parameters={\n",
        "          \"W1\":np.random.randn(input_layer,hidden_layer)*np.sqrt(2/input_layer), #[784,300]\n",
        "          'b1':np.zeros((1,hidden_layer)), #[1,300]\n",
        "          \"W2\":np.random.randn(hidden_layer,output_layer)*np.sqrt(2/hidden_layer), #[300,10]\n",
        "          \"b2\":np.zeros((1,output_layer)) #[1,10]\n",
        "            }\n",
        "#print(parameters[\"W1\"].shape)\n",
        "\n",
        "def sigmoid(x):\n",
        "  s = 1. / (1. + np.exp(-x))\n",
        "  return s\n",
        "#Feedforward implementation\n",
        "\"\"\"In the previous problem for the output layer is used score function,hence couldnot use the one hot encoding.\n",
        "In this for the output layer we use the softmax function where the score functions are exponentiated and normalized \n",
        "probability used which will give us a intuitive data.\n",
        "\"\"\"\n",
        "def feed_forward(X,parameters):\n",
        "  activations={}\n",
        "\n",
        "  activations[\"Z1\"]=np.dot(X,parameters[\"W1\"])+parameters[\"b1\"] #[60000,784]*[784,300]=[60000,300]\n",
        "  activations[\"A1\"]=sigmoid(activations[\"Z1\"]) #[60000,300]\n",
        "  activations[\"Z2\"]=np.dot(activations[\"A1\"],parameters[\"W2\"])+parameters[\"b2\"] #[60000,300]*[300,10]=[60000,10] scores for the output layers\n",
        "  \n",
        "  exp_scores=np.exp(activations[\"Z2\"])\n",
        "  exp_sum=np.sum(exp_scores,axis=1,keepdims=True) #exp_sum=[60000,1] without keepdimension its[60000,] and we then can't do dot product\n",
        "  activations[\"A2\"]=exp_scores/exp_sum #[60000,10]\n",
        "  #print(activations[\"A2\"].shape)\n",
        "  return activations\n",
        "def loss(Y,output):\n",
        "  Loss_sum=np.sum(np.multiply(Y,np.log(output)))\n",
        "  data_loss= -(1/num_labels) * Loss_sum\n",
        "\n",
        "  #reg_loss= 0.5*reg*np.sum(parameters[\"W1\"]*parameters[\"W1\"]) + 0.5*reg*np.sum(parameters[\"W2\"]*parameters[\"W2\"])\n",
        "\n",
        "  #loss=data_loss + reg_loss\n",
        "\n",
        "  return data_loss\n",
        "\n",
        "def backpropagation(X,Y,parameters,activations):\n",
        "\n",
        "  dZ2=activations[\"A2\"]-Y #[60000,10]\n",
        "  #print(dZ2[100,:])\n",
        "  dW2=(1/mini_batch)*np.dot(activations[\"A1\"].T,dZ2) #[300,10]\n",
        "  db2=(1/mini_batch)*np.sum(dZ2,axis=0,keepdims=True)#[1,10]\n",
        "  dA1=np.dot(dZ2,parameters[\"W2\"].T) #[60000,300]\n",
        "  dZ1=dA1 * sigmoid(activations[\"Z1\"]) * (1-sigmoid(activations[\"Z1\"])) #[60000,300]\n",
        "  dW1=(1/mini_batch)*np.dot(X.T,dZ1)#[784,300]\n",
        "  db1=(1/mini_batch)*np.sum(dZ1,axis=0,keepdims=True)#[1,300]\n",
        "\n",
        "  gradients={\n",
        "      \"dW1\":dW1,\n",
        "      \"db1\":db1,\n",
        "      \"dW2\":dW2,\n",
        "      \"db2\":db2\n",
        "            }\n",
        "  return gradients\n",
        "\n",
        "#Training the data with mini-batch gradient descent\n",
        "for i in range(epoch):  \n",
        "  \n",
        "\n",
        "#shuffling the training set\n",
        "  permutation = np.random.permutation(num_examples) #the sequence is altered randomly\n",
        "  train_images_shuffled = train_images[permutation,:]\n",
        "  train_labels_shuffled = train_labels[permutation,:]\n",
        "\n",
        "\n",
        "  for j in range(batches):\n",
        "    begin=j*mini_batch_size\n",
        "    end=min(begin+mini_batch_size,num_examples-1)\n",
        "    X=train_images_shuffled[begin:end,:]\n",
        "    Y=train_labels_shuffled[begin:end]\n",
        "    mini_batch = end - begin\n",
        "\n",
        "    activations=feed_forward(X,parameters)\n",
        "    gradients=backpropagation(X,Y,parameters,activations)\n",
        "                                                                                                                                                                                                      \n",
        "    parameters[\"W1\"]+= -step_size*gradients[\"dW1\"]\n",
        "    parameters[\"b1\"]+= -step_size*gradients[\"db1\"]\n",
        "    parameters[\"W2\"]+= -step_size*gradients[\"dW2\"]\n",
        "    parameters[\"b2\"]+= -step_size*gradients[\"db2\"]\n",
        "  \n",
        "  activations=feed_forward(train_images,parameters)\n",
        "  train_loss=loss(train_labels,activations[\"A2\"])\n",
        "\n",
        "  activations=feed_forward(test_images,parameters)\n",
        "  test_loss=loss(test_labels,activations[\"A2\"])\n",
        "  print(\"Epoch{}: training cost= {} and testing loss={} \".format(i,train_loss,test_loss))\n",
        "\n",
        "print(\"Done\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch0: training cost= 0.2353732342481926 and testing loss=0.039565654655020266 \n",
            "Epoch1: training cost= 0.17550297496158399 and testing loss=0.0306824635652809 \n",
            "Epoch2: training cost= 0.15402047166823332 and testing loss=0.02838897378744749 \n",
            "Epoch3: training cost= 0.1509742962156808 and testing loss=0.029811878792046456 \n",
            "Epoch4: training cost= 0.12006342184134833 and testing loss=0.024801344800814955 \n",
            "Epoch5: training cost= 0.11858124048219774 and testing loss=0.025008602451170765 \n",
            "Epoch6: training cost= 0.13431273335456861 and testing loss=0.02757513053231888 \n",
            "Epoch7: training cost= 0.09995059119637906 and testing loss=0.023618414787580855 \n",
            "Epoch8: training cost= 0.10865535027484804 and testing loss=0.02569616925689162 \n",
            "Epoch9: training cost= 0.09721743594556893 and testing loss=0.023994455750857773 \n",
            "Epoch10: training cost= 0.08526813886803757 and testing loss=0.02335808162084288 \n",
            "Epoch11: training cost= 0.07373117303301113 and testing loss=0.021408733971826136 \n",
            "Epoch12: training cost= 0.07320412686925618 and testing loss=0.02180851071343705 \n",
            "Epoch13: training cost= 0.07764208356704656 and testing loss=0.022875922682467943 \n",
            "Epoch14: training cost= 0.06656400360585836 and testing loss=0.02183053519421065 \n",
            "Epoch15: training cost= 0.06883552872344031 and testing loss=0.022925416390323125 \n",
            "Epoch16: training cost= 0.07226918146996188 and testing loss=0.023110204405634018 \n",
            "Epoch17: training cost= 0.053327864329233046 and testing loss=0.02116765562070383 \n",
            "Epoch18: training cost= 0.05333968881978735 and testing loss=0.020689866912535364 \n",
            "Epoch19: training cost= 0.04795232470427554 and testing loss=0.020433908548515147 \n",
            "Epoch20: training cost= 0.051024184223875876 and testing loss=0.02241864233522414 \n",
            "Epoch21: training cost= 0.04906844522965499 and testing loss=0.023544451190264052 \n",
            "Epoch22: training cost= 0.047571756307681554 and testing loss=0.021133159237082352 \n",
            "Epoch23: training cost= 0.04670322244349932 and testing loss=0.021631557015478618 \n",
            "Epoch24: training cost= 0.045144751821819144 and testing loss=0.021366210096762008 \n",
            "Epoch25: training cost= 0.03843934368207314 and testing loss=0.02246532314432609 \n",
            "Epoch26: training cost= 0.03256884504126941 and testing loss=0.021423485347031663 \n",
            "Epoch27: training cost= 0.04162162496533132 and testing loss=0.022420320378217715 \n",
            "Epoch28: training cost= 0.04047762422180349 and testing loss=0.022403781044385545 \n",
            "Epoch29: training cost= 0.03425106957719367 and testing loss=0.0214856605246169 \n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQcXo1mYXhwV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6b360d2f-efce-4f8b-b681-fdc565a2d558"
      },
      "source": [
        "activations=feed_forward(train_images,parameters)\n",
        "predicted_class = np.argmax(activations[\"A2\"],axis=1)\n",
        "print(predicted_class[800])\n",
        "predicted_class_encoded=np.eye(digits)[predicted_class.astype('int32')]\n",
        "print(predicted_class_encoded[800,:])\n",
        "print(\"training accuracy:%.6f\"%(np.mean(predicted_class_encoded==train_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "training accuracy:0.998007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvgiJ1WjtRM1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "531b86fd-0e10-42d2-fe21-aa8185d3dc21"
      },
      "source": [
        "activations=feed_forward(test_images,parameters)\n",
        "predicted_class = np.argmax(activations[\"A2\"],axis=1)\n",
        "print(predicted_class[800])\n",
        "\n",
        "predicted_class_encoded=np.eye(digits)[predicted_class.astype('int32')]\n",
        "print(predicted_class_encoded[800,:])\n",
        "print(test_labels[800,:])\n",
        "print(\"testing accuracy:%.5f\"%(np.mean(predicted_class_encoded==test_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "testing accuracy:0.99324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3mfcm5mxnap"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}